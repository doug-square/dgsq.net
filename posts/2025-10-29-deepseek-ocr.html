<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="color-scheme" content="dark light">
    <meta name="robots" content="noindex, nofollow, noai, noimageai">
    <meta name="darkreader-lock">
    <meta name="generator" content="my own :3">
    <meta name="og:type" content="website">
    <meta name="og:url" content="">
    <meta name="og:title" content="Nyaaaaaaaa">
    <meta name="og:site_name" content="lunahd dot neocities">
    <meta name="og:description" content="Miku's website">
    <meta name="description" content="Miku's website">
    <title>dgsq</title>
    <link rel="stylesheet" href="/style.css">
    <!-- <link rel="icon" href="icon.png"> -->
  </head>
  <body>
    <nav id="header">
      <div id="nav-links">
        <a href="/">home</a>
        <a href="/posts">posts</a>
      </div>
    </nav>
    <div id="main-container">
      <div id="sidebar">
        <div id="links-general">
          <div class="sb-header">
            <h2>personal links</h2>
          </div>
          <div class="sb-box">
            <ul>
              <li><a href="https://github.com/doug-square">github</a></li>
              <li><a href="https://mastodon.social/@dgsq">mastodon</a></li>
            </ul>
            (If there's nothing there it's because I do too much of my work off the public radar)
          </div>
        </div>
        <div id="blogs">
          <div class="sb-header">
            <h2>blogs</h2>
          </div>
          <div class="sb-box">
            <ul>
              <li><a href="https://www.astralcodexten.com/">Astral Codex Ten</a></li>
              <li><a href="https://tomverbeure.github.io/">Electronics etc...</a></li>
              <li><a href="https://ludic.mataroa.blog/">Ludicity</a></li>
              <li><a href="https://scottaaronson.blog/">Shtetl-Optimized</a></li>
              <li><a href="http://radar.spacebar.org/">Tom 7 Radar</a></li>
            </ul>
          </div>
        </div>
        <div id="contact">
          <div class="sb-header">
            <h2>contact</h2>
          </div>
          <div class="sb-box">
            Maybe I'll add a full suite of options here eventually, but
            for now, you can <a href="mailto:dgsq@dgsq.net">email me</a>.
          </div>
        </div>
      </div>
      <article id="content">
<ul class="blog-title">
    <li><h1>Some thoughts on Deepseek-OCR</h1></li>
    <li></li>
    <li><h4>October 28, 2025</h4>
</ul>

<p>
    Recently, DeepSeek released a <a href="https://github.com/deepseek-ai/DeepSeek-OCR">new model and paper</a>, titled DeepSeek-OCR.
    I've seen a bunch of people talk about it recently, and it's pretty cool stuff.
</p>

<p>
    I love the idea of using images to compress text.
    As <a href="https://x.com/karpathy/status/1980397031542989305">Andrej points out</a>, the tokenizer is kind of ugly, and it
    would be cool to have a more natural system. Intuitively,
    it makes sense that a more optimal way to parse written letters/characters might involve some amount of visual processing, since
    that's sort of how humans do it, with many people pattern-matching several words (or more) at a time when reading quickly, instead of
    parsing text as individual letters/tokens.
</p>

<p>
    What I've been trying to figure out is how suprising this compression rate should be. From their abstract:
    <blockquote>
Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a
compression ratio &lt; 10x), the model can achieve decoding (OCR) precision of 97%. Even at a
compression ratio of 20x, the OCR accuracy still remains at about 60%.
    </blockquote>
</p>
<p>
    This sounds pretty good, but there are other ways to compress transformers. DeepSeek-OCR was released using BF16 weights, and since
    they didn't mention anything about their data precision in the paper, I'm assuming that's what they evaluated on.
</p>

<p>
    On the other hand, weight quantization in the form of low-precision inference has been making substantial progress.
    I can't find much data on <a href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/">NVFP4</a>,
    but it seems to promise 4-<i>ish</i> bits per weight and activation (a bit more due to scaling factors) for only slightly reduced accuracy.
    This means that the 10x-20x advantage claimed by the DeepSeek-OCR paper may just be a 2.5-5x advantage over a 4-bit quantized model.
    I would be very curious to see whether the image-token-to-text-token compression would remain as high as it is when this model is
    quantized to such low precision. Perhaps image tokens are just able to use this "extra space" better than text tokens.
</p>

<p>
    Taking this a step further: <i>if</i> this advantage really does decrease from 10x-20x to 2.5x-5x when the model is quantized to 4 bit,
    then this might imply that text tokens should really be quantized to between 0.8 to 1.6 bits. It seems like a somewhat pleasing result
    that this line of reasoning should land in the vicinity of 1-bit quantization.
</p>

<p>
    On the other hand, it is also interesting to ask which would better for performance: many 1-bit text tokens or 10-20x fewer 16 bit tokens.
    Or maybe something in the middle? It seems clear to me that fewer high-precision tokens would be desirable for low-latency decode, given
    its autoregressive nature, especially given that current GPU hardware targets 16-bit computations. Maybe the tradeoff would be different
    in other circumstances though.
</p>

<p>
    Anyways, one more disclaimer that this has all been very speculative. I have not carried out any experiments of my own here.
</p>
      </article>
    </div>
    <br>
    <div id="after">
      I used to have a generic-looking website made in Jekyll, but then I saw <a href="https://lunahd.neocities.org/">Miku's</a> and stole hers (<a href="https://github.com/doug-square/dgsq.net">source</a>).
    </div>
  </body>
</html>
